import time
import logging
from google import genai
from google.genai import types
from config import AppConfig
from models import APIError


class GeminiService:
    def __init__(self, config: AppConfig):
        self.config = config
        self.genai_clients = [genai.Client(api_key=key) for key in config.google_api_keys]
        self.current_genai_index = 0

    def generate_summary(self, text: str) -> str:
        """ç”Ÿæˆæ–‡å­—æ‘˜è¦ - è¶…é•·éŒ„éŸ³æ™ºèƒ½è™•ç†ç‰ˆæœ¬"""
        start_time = time.time()

        try:
            text_length = len(text)
            logging.info(f"é–‹å§‹è™•ç†æ–‡å­—æ‘˜è¦ï¼Œé•·åº¦: {text_length} å­—ç¬¦")

            # ä¼°ç®—éŒ„éŸ³æ™‚é•·ï¼ˆç²—ç•¥ä¼°ç®—ï¼šæ¯åˆ†é˜ç´„150-200å­—ï¼‰
            estimated_minutes = text_length / 180
            
            if text_length <= 1500:
                # çŸ­éŒ„éŸ³ï¼ˆ<10åˆ†é˜ï¼‰ï¼šå®Œæ•´æ‘˜è¦
                return self._generate_complete_summary(text)
            elif text_length <= 5000:
                # ä¸­ç­‰éŒ„éŸ³ï¼ˆ10-30åˆ†é˜ï¼‰ï¼šé‡é»æ‘˜è¦
                return self._generate_focused_summary(text)
            elif text_length <= 15000:
                # é•·éŒ„éŸ³ï¼ˆ30åˆ†é˜-1.5å°æ™‚ï¼‰ï¼šçµæ§‹åŒ–æ‘˜è¦
                return self._generate_structured_summary(text)
            else:
                # è¶…é•·éŒ„éŸ³ï¼ˆ>1.5å°æ™‚ï¼‰ï¼šåˆ†æ®µå¼æ‘˜è¦
                return self._generate_segmented_summary(text, estimated_minutes)

        except Exception as e:
            processing_time = time.time() - start_time
            logging.error(f"Gemini è™•ç†å¤±æ•— (è€—æ™‚{processing_time:.2f}ç§’): {e}")
            return "æ‘˜è¦åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œä½†éŒ„éŸ³è½‰æ–‡å­—æˆåŠŸã€‚"

    def _generate_complete_summary(self, text: str) -> str:
        """å®Œæ•´æ‘˜è¦ï¼ˆçŸ­éŒ„éŸ³ï¼‰"""
        prompt = f"è«‹å°‡ä»¥ä¸‹éŒ„éŸ³å…§å®¹æ•´ç†æˆé‡é»æ‘˜è¦ï¼š\n\n{text}"
        
        config = types.GenerateContentConfig(
            temperature=0.1,
            max_output_tokens=60000,
            top_p=0.8,
            top_k=10
        )
        
        response = self._call_gemini_with_rotation(prompt, config)
        return self._extract_response_text(response, text)

    def _generate_focused_summary(self, text: str) -> str:
        """é‡é»æ‘˜è¦ï¼ˆä¸­ç­‰éŒ„éŸ³ï¼‰"""
        try:
            logging.info("ä½¿ç”¨é‡é»æ‘˜è¦æ¨¡å¼è™•ç†ä¸­ç­‰é•·åº¦éŒ„éŸ³")
            prompt = f"è«‹å°‡ä»¥ä¸‹éŒ„éŸ³å…§å®¹æ•´ç†æˆé‡é»æ‘˜è¦ï¼Œçªå‡ºä¸»è¦è§€é»å’Œé—œéµè³‡è¨Šï¼š\n\n{text}"
            
            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=60000,
                top_p=0.8,
                top_k=10
            )
            
            response = self._call_gemini_with_rotation(prompt, config)
            result = self._extract_response_text(response, text)
            
            logging.info(f"é‡é»æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•·åº¦: {len(result)} å­—ç¬¦")
            return result
            
        except Exception as e:
            logging.error(f"é‡é»æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
            # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦æ›´ç°¡å–®çš„è™•ç†æ–¹å¼
            return self._generate_simple_focused_summary(text)

    def _generate_structured_summary(self, text: str) -> str:
        """çµæ§‹åŒ–æ‘˜è¦ï¼ˆé•·éŒ„éŸ³ï¼‰"""
        # å°‡æ–‡å­—åˆ†æˆ3æ®µé€²è¡Œåˆ†æ
        length = len(text)
        segment1 = text[:length//3]
        segment2 = text[length//3:2*length//3]
        segment3 = text[2*length//3:]
        
        prompt = f"""è«‹åˆ†æä»¥ä¸‹è¼ƒé•·éŒ„éŸ³çš„å…§å®¹ï¼Œæä¾›çµæ§‹åŒ–æ‘˜è¦ï¼š

ã€å‰æ®µå…§å®¹ã€‘
{segment1[:2000]}

ã€ä¸­æ®µå…§å®¹ã€‘
{segment2[:2000]}

ã€å¾Œæ®µå…§å®¹ã€‘ 
{segment3[:2000]}

è«‹æä¾›ï¼š
1. ä¸»è¦ä¸»é¡Œ
2. é‡é»å…§å®¹
3. é—œéµçµè«–
4. é‡è¦ç´°ç¯€"""

        config = types.GenerateContentConfig(
            temperature=0.1,
            max_output_tokens=60000,
            top_p=0.8,
            top_k=10
        )
        
        response = self._call_gemini_with_rotation(prompt, config)
        result = self._extract_response_text(response, text, structured=True)
        
        return f"{result}\n\nğŸ“Š éŒ„éŸ³æ™‚é•·ï¼šç´„ {len(text)/180:.0f} åˆ†é˜"

    def _generate_segmented_summary(self, text: str, estimated_minutes: float) -> str:
        """åˆ†æ®µå¼æ‘˜è¦ï¼ˆè¶…é•·éŒ„éŸ³ï¼‰"""
        try:
            # å°‡æ–‡å­—åˆ†æˆå¤šå€‹æ®µè½ï¼Œæ¯æ®µç´„3000å­—
            segments = []
            chunk_size = 3000
            for i in range(0, len(text), chunk_size):
                segment = text[i:i+chunk_size]
                segments.append(segment)
            
            logging.info(f"è¶…é•·éŒ„éŸ³åˆ†ç‚º {len(segments)} æ®µè™•ç†")
            
            # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é€²è¡Œå®Œæ•´åˆ†æ
            if self.config.full_analysis:
                # å®Œæ•´åˆ†ææ‰€æœ‰æ®µè½
                if len(segments) <= self.config.max_segments_for_full_analysis:
                    key_segments = segments
                    analysis_note = f"ï¼ˆå®Œæ•´åˆ†æ {len(segments)} æ®µï¼‰"
                    logging.info(f"é€²è¡Œå®Œæ•´åˆ†æï¼Œè™•ç† {len(segments)} æ®µ")
                else:
                    # å¦‚æœæ®µè½æ•¸è¶…éé™åˆ¶ï¼Œé€²è¡Œè­¦å‘Šä½†ä»ç›¡å¯èƒ½åˆ†ææ›´å¤š
                    key_segments = segments[:self.config.max_segments_for_full_analysis]
                    analysis_note = f"ï¼ˆå› æ®µè½éå¤šï¼Œå·²åˆ†æå‰ {len(key_segments)} æ®µï¼Œå…± {len(segments)} æ®µï¼‰"
                    logging.warning(f"æ®µè½æ•¸ {len(segments)} è¶…éé™åˆ¶ {self.config.max_segments_for_full_analysis}ï¼Œåªåˆ†æå‰ {len(key_segments)} æ®µ")
            else:
                # æ™ºèƒ½é¸å–é—œéµæ®µè½ï¼ˆåŸæœ‰é‚è¼¯ï¼‰
                if len(segments) > 10:
                    # å–é–‹é ­3æ®µã€ä¸­é–“2æ®µã€çµå°¾3æ®µ
                    key_segments = segments[:3] + segments[len(segments)//2-1:len(segments)//2+1] + segments[-3:]
                    analysis_note = f"ï¼ˆæ™ºèƒ½é¸å–ï¼šå·²å¾ {len(segments)} æ®µä¸­é¸å– {len(key_segments)} å€‹é—œéµæ®µè½åˆ†æï¼‰"
                else:
                    key_segments = segments[:6]  # æœ€å¤šè™•ç†å‰6æ®µ
                    analysis_note = f"ï¼ˆå…± {len(segments)} æ®µï¼Œå·²åˆ†æå‰ {len(key_segments)} æ®µï¼‰"
            
            # ç”Ÿæˆåˆ†æ®µæ‘˜è¦
            segment_summaries = []
            total_segments = len(key_segments)
            
            # å¦‚æœæ˜¯å®Œæ•´åˆ†æä¸”æ®µè½å¾ˆå¤šï¼Œç™¼é€é€²åº¦é€šçŸ¥
            if self.config.full_analysis and total_segments > 20:
                logging.info(f"é–‹å§‹å®Œæ•´åˆ†æ {total_segments} æ®µï¼Œé è¨ˆéœ€è¦ {total_segments * 0.5:.0f} ç§’")
            
            for i, segment in enumerate(key_segments):
                try:
                    # å‹•æ…‹èª¿æ•´æ®µè½æ¨™è¨˜ï¼ˆå¦‚æœæ˜¯æ™ºèƒ½é¸å–ï¼Œä½¿ç”¨åŸå§‹æ®µè½è™Ÿï¼‰
                    if self.config.full_analysis or len(segments) <= 10:
                        segment_label = f"ç¬¬{i+1}æ®µ"
                    else:
                        # æ™ºèƒ½é¸å–æ¨¡å¼ï¼Œè¨ˆç®—åŸå§‹æ®µè½è™Ÿ
                        if i < 3:
                            segment_number = i + 1
                        elif i < 5:
                            segment_number = len(segments)//2 + (i - 3)
                        else:
                            segment_number = len(segments) - (7 - i)
                        segment_label = f"ç¬¬{segment_number}æ®µ"
                    
                    prompt = f"è«‹ç°¡æ½”ç¸½çµä»¥ä¸‹éŒ„éŸ³ç‰‡æ®µçš„é‡é»ï¼ˆ{segment_label}ï¼‰ï¼š\n\n{segment[:2000]}"
                    
                    config = types.GenerateContentConfig(
                        temperature=0.1,
                        max_output_tokens=10000,
                        top_p=0.8,
                        top_k=5
                    )
                    
                    response = self._call_gemini_with_rotation(prompt, config)
                    if response and response.candidates:
                        summary = response.text.strip()
                        segment_summaries.append(f"ã€{segment_label}ã€‘{summary}")
                    
                    # è¨˜éŒ„è™•ç†é€²åº¦
                    if (i + 1) % 10 == 0:
                        logging.info(f"å·²å®Œæˆ {i + 1}/{total_segments} æ®µåˆ†æ")
                    
                    time.sleep(self.config.segment_processing_delay)  # ä½¿ç”¨é…ç½®çš„å»¶é²æ™‚é–“
                    
                except Exception as e:
                    logging.warning(f"è™•ç†{segment_label}æ™‚å‡ºéŒ¯: {e}")
                    segment_summaries.append(f"ã€{segment_label}ã€‘è™•ç†å¤±æ•—")
            
            # ç”Ÿæˆç¸½é«”æ‘˜è¦
            combined_summary = "\n\n".join(segment_summaries)
            
            final_prompt = f"""åŸºæ–¼ä»¥ä¸‹åˆ†æ®µæ‘˜è¦ï¼Œè«‹æä¾›æ•´é«”é‡é»ç¸½çµï¼š

{combined_summary}

è«‹æä¾›ï¼š
1. ä¸»è¦è­°é¡Œå’Œä¸»é¡Œ
2. æ ¸å¿ƒè§€é»å’Œçµè«–
3. é‡è¦æ±ºå®šæˆ–è¡Œå‹•é …ç›®"""

            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=60000,
                top_p=0.8,
                top_k=10
            )
            
            final_response = self._call_gemini_with_rotation(final_prompt, config)
            final_summary = self._extract_response_text(final_response, text, structured=True)
            
            # çµ„åˆæœ€çµ‚çµæœ
            result = f"ğŸ¯ ã€æ•´é«”æ‘˜è¦ã€‘\n{final_summary}\n\nğŸ“ ã€åˆ†æ®µé‡é»ã€‘\n{combined_summary}\n\n"
            result += f"â±ï¸ éŒ„éŸ³æ™‚é•·ï¼šç´„ {estimated_minutes:.0f} åˆ†é˜ ({len(text)} å­—)\n"
            result += f"ğŸ“Š åˆ†æèªªæ˜ï¼š{analysis_note}"
            
            return result
            
        except Exception as e:
            logging.error(f"åˆ†æ®µæ‘˜è¦è™•ç†å¤±æ•—: {e}")
            return self._generate_fallback_summary(text, estimated_minutes)

    def _generate_fallback_summary(self, text: str, estimated_minutes: float) -> str:
        """å‚™ç”¨æ‘˜è¦ï¼ˆç•¶åˆ†æ®µè™•ç†å¤±æ•—æ™‚ï¼‰"""
        # åªå–é–‹é ­å’Œçµå°¾é€²è¡Œæ‘˜è¦
        start_text = text[:2000]
        end_text = text[-2000:] if len(text) > 4000 else ""
        
        summary_text = f"é–‹é ­ï¼š{start_text}"
        if end_text:
            summary_text += f"\n\nçµå°¾ï¼š{end_text}"
        
        prompt = f"é€™æ˜¯ä¸€å€‹ç´„ {estimated_minutes:.0f} åˆ†é˜çš„é•·éŒ„éŸ³çš„é–‹é ­å’Œçµå°¾éƒ¨åˆ†ï¼Œè«‹æä¾›åŸºæœ¬æ‘˜è¦ï¼š\n\n{summary_text}"
        
        try:
            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=30000,
                top_p=0.8,
                top_k=5
            )
            
            response = self._call_gemini_with_rotation(prompt, config)
            result = self._extract_response_text(response, text)
            
            return f"{result}\n\nâš ï¸ å› éŒ„éŸ³éé•·ï¼Œæ­¤ç‚ºç°¡åŒ–æ‘˜è¦\nâ±ï¸ éŒ„éŸ³æ™‚é•·ï¼šç´„ {estimated_minutes:.0f} åˆ†é˜"
            
        except Exception as e:
            logging.error(f"å‚™ç”¨æ‘˜è¦ä¹Ÿå¤±æ•—: {e}")
            return f"âœ… éŒ„éŸ³è½‰æ–‡å­—æˆåŠŸ\nâ±ï¸ éŒ„éŸ³æ™‚é•·ï¼šç´„ {estimated_minutes:.0f} åˆ†é˜ ({len(text)} å­—)\nğŸ“ å› å…§å®¹éé•·ï¼Œæ‘˜è¦åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œè«‹æŸ¥çœ‹å®Œæ•´é€å­—ç¨¿"

    def _extract_response_text(self, response, original_text: str, structured: bool = False) -> str:
        """æå–å›æ‡‰æ–‡å­—ä¸¦è™•ç†å„ç¨®ç‹€æ³"""
        if not response or not response.candidates:
            logging.warning("Gemini å›æ‡‰ç„¡å…§å®¹æˆ–ç„¡å€™é¸é …")
            raise APIError("ç„¡æ³•ç”Ÿæˆæ‘˜è¦å›æ‡‰")
        
        candidate = response.candidates[0]
        finish_reason = str(candidate.finish_reason)
        
        logging.info(f"Gemini å›æ‡‰ç‹€æ…‹: {finish_reason}")
        
        if "STOP" in finish_reason:
            result = response.text.strip()
            logging.info(f"æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•·åº¦: {len(result)} å­—ç¬¦")
            return result
        elif "SAFETY" in finish_reason:
            return "âš ï¸ å…§å®¹å¯èƒ½åŒ…å«æ•æ„Ÿè³‡è¨Šï¼Œç„¡æ³•ç”¢ç”Ÿæ‘˜è¦"
        elif "MAX_TOKEN" in finish_reason or "LENGTH" in finish_reason:
            logging.warning(f"Token é™åˆ¶è§¸ç™¼: {finish_reason}")
            # å¦‚æœæ˜¯çµæ§‹åŒ–è™•ç†ï¼Œå˜—è©¦è¿”å›éƒ¨åˆ†çµæœ
            if structured and response.text:
                return f"{response.text.strip()}\n\nâš ï¸ æ‘˜è¦å› é•·åº¦é™åˆ¶å¯èƒ½ä¸å®Œæ•´"
            else:
                # å°æ–¼ä¸­ç­‰é•·åº¦éŒ„éŸ³ï¼Œå˜—è©¦ç°¡åŒ–è™•ç†
                raise APIError(f"å…§å®¹éé•·éœ€è¦ç°¡åŒ–è™•ç†: {finish_reason}")
        else:
            logging.warning(f"æœªçŸ¥çš„å®Œæˆç‹€æ…‹: {finish_reason}")
            if response.text and len(response.text.strip()) > 0:
                return f"{response.text.strip()}\n\nâš ï¸ æ‘˜è¦å¯èƒ½ä¸å®Œæ•´ï¼ˆ{finish_reason}ï¼‰"
            else:
                raise APIError(f"æ‘˜è¦ç”Ÿæˆç•°å¸¸: {finish_reason}")

    def _generate_simple_focused_summary(self, text: str) -> str:
        """ç°¡åŒ–ç‰ˆé‡é»æ‘˜è¦ï¼ˆä¸­ç­‰éŒ„éŸ³å‚™ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            logging.info("ä½¿ç”¨ç°¡åŒ–ç‰ˆé‡é»æ‘˜è¦")
            # åˆ†æ®µè™•ç†ï¼Œæ¯æ®µ2000å­—ç¬¦
            chunks = [text[i:i+2000] for i in range(0, len(text), 2000)]
            
            summaries = []
            for i, chunk in enumerate(chunks[:3]):  # æœ€å¤šè™•ç†å‰3æ®µ
                try:
                    prompt = f"è«‹ç°¡æ½”ç¸½çµä»¥ä¸‹å…§å®¹çš„é‡é»ï¼š\n\n{chunk}"
                    
                    config = types.GenerateContentConfig(
                        temperature=0.1,
                        max_output_tokens=20000,
                        top_p=0.8,
                        top_k=5
                    )
                    
                    response = self._call_gemini_with_rotation(prompt, config)
                    if response and response.candidates and "STOP" in str(response.candidates[0].finish_reason):
                        summaries.append(response.text.strip())
                    
                    time.sleep(0.3)  # çŸ­æš«å»¶é²
                    
                except Exception as e:
                    logging.warning(f"è™•ç†ç¬¬{i+1}æ®µç°¡åŒ–æ‘˜è¦å¤±æ•—: {e}")
                    continue
            
            if summaries:
                result = "\n\n".join(summaries)
                if len(chunks) > 3:
                    result += f"\n\nğŸ’¡ è¨»ï¼šå·²æ‘˜è¦å‰3æ®µå…§å®¹ï¼Œç¸½å…±{len(chunks)}æ®µ"
                return result
            else:
                return self._generate_short_summary(text[:1000])
                
        except Exception as e:
            logging.error(f"ç°¡åŒ–ç‰ˆé‡é»æ‘˜è¦å¤±æ•—: {e}")
            return self._generate_short_summary(text[:1000])

    def _generate_short_summary(self, text: str) -> str:
        """ç”Ÿæˆç°¡çŸ­æ‘˜è¦ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            logging.info("ä½¿ç”¨ç°¡çŸ­æ‘˜è¦æ¨¡å¼")
            prompt = f"è«‹ç”¨æœ€ç°¡æ½”çš„æ–¹å¼ç¸½çµä»¥ä¸‹å…§å®¹çš„ä¸»è¦é‡é»ï¼ˆé™100å­—å…§ï¼‰ï¼š\n\n{text[:1000]}"
            
            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=20000,
                top_p=0.8,
                top_k=5
            )

            response = self._call_gemini_with_rotation(prompt, config)
            
            if response and response.candidates and "STOP" in str(response.candidates[0].finish_reason):
                return f"{response.text.strip()}\n\nâš ï¸ å› è™•ç†é™åˆ¶ï¼Œæ­¤ç‚ºç°¡åŒ–æ‘˜è¦"
            else:
                return "âœ… éŒ„éŸ³è½‰æ–‡å­—æˆåŠŸ\nğŸ“ å…§å®¹è¼ƒé•·ï¼Œå»ºè­°æŸ¥çœ‹å®Œæ•´é€å­—ç¨¿"
                
        except Exception as e:
            logging.error(f"ç°¡çŸ­æ‘˜è¦ä¹Ÿå¤±æ•—: {e}")
            return "âœ… éŒ„éŸ³è½‰æ–‡å­—æˆåŠŸ\nğŸ“ æ‘˜è¦åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œè«‹æŸ¥çœ‹å®Œæ•´é€å­—ç¨¿"

    def _call_gemini_with_rotation(self, prompt: str, config: types.GenerateContentConfig):
        """å¿«é€Ÿè¼ªè©¢APIé‡‘é‘°ï¼Œåªå˜—è©¦ä¸€æ¬¡"""
        client = self.genai_clients[self.current_genai_index]
        try:
            response = client.models.generate_content(
                model=self.config.gemini_model,
                contents=prompt,
                config=config
            )
            return response
        except Exception as e:
            logging.warning(f"Gemini API é‡‘é‘° {self.current_genai_index + 1} å¤±æ•—: {e}")
            # åˆ‡æ›åˆ°ä¸‹ä¸€å€‹é‡‘é‘°ä¾›ä¸‹æ¬¡ä½¿ç”¨
            self.current_genai_index = (self.current_genai_index + 1) % len(self.genai_clients)
            raise APIError(f"Gemini API èª¿ç”¨å¤±æ•—: {e}") 