"""
ç•°æ­¥Google Gemini AIæ‘˜è¦æœå‹™
"""

import logging
import asyncio
import time
from typing import Dict, Any, Optional, List
import json
import random
from google import genai
from google.genai import types

from config import AppConfig

logger = logging.getLogger(__name__)


class AsyncGeminiService:
    """ç•°æ­¥Google Gemini AIæœå‹™"""
    
    def __init__(self, config: AppConfig):
        self.config = config
        self.api_keys = config.google_api_keys
        self.model = config.gemini_model
        self.thinking_budget = config.thinking_budget
        self.max_retries = config.max_retries
        
        if not self.api_keys:
            raise ValueError("Google APIå¯†é‘°æœªè¨­ç½®")
        
        # åˆå§‹åŒ–æ¯å€‹APIé‡‘é‘°çš„å®¢æˆ¶ç«¯
        self.genai_clients = [genai.Client(api_key=key) for key in self.api_keys]
        self.current_genai_index = 0
    
    async def generate_summary(self, transcript: str) -> str:
        """
        ç”Ÿæˆæ–‡å­—æ‘˜è¦ - æ”¯æ´ä¸åŒé•·åº¦æ–‡æœ¬çš„æ™ºèƒ½è™•ç†
        
        Args:
            transcript: è½‰éŒ„çš„æ–‡å­—
            
        Returns:
            æ‘˜è¦æ–‡å­—
        """
        start_time = time.time()

        try:
            logger.info("ğŸ¤– é–‹å§‹ç”ŸæˆAIæ‘˜è¦")
            
            if not transcript or not transcript.strip():
                raise ValueError("è½‰éŒ„æ–‡å­—ç‚ºç©º")
            
            text_length = len(transcript)
            logger.info(f"é–‹å§‹è™•ç†æ–‡å­—æ‘˜è¦ï¼Œé•·åº¦: {text_length} å­—ç¬¦")

            # ä¼°ç®—éŒ„éŸ³æ™‚é•·ï¼ˆç²—ç•¥ä¼°ç®—ï¼šæ¯åˆ†é˜ç´„150-200å­—ï¼‰
            estimated_minutes = text_length / 180
            
            # æ ¹æ“šæ–‡æœ¬é•·åº¦é¸æ“‡ä¸åŒçš„è™•ç†ç­–ç•¥
            if text_length <= 3000:
                # çŸ­éŒ„éŸ³ï¼ˆ<10åˆ†é˜ï¼‰ï¼šå®Œæ•´æ‘˜è¦
                summary = await self._generate_complete_summary(transcript)
            elif text_length <= 6000:
                # ä¸­ç­‰éŒ„éŸ³ï¼ˆ10-30åˆ†é˜ï¼‰ï¼šé‡é»æ‘˜è¦
                summary = await self._generate_focused_summary(transcript)
            elif text_length <= 18000:
                # é•·éŒ„éŸ³ï¼ˆ30åˆ†é˜-1.5å°æ™‚ï¼‰ï¼šçµæ§‹åŒ–æ‘˜è¦
                summary = await self._generate_structured_summary(transcript)
            else:
                # è¶…é•·éŒ„éŸ³ï¼ˆ>1.5å°æ™‚ï¼‰ï¼šåˆ†æ®µå¼æ‘˜è¦
                summary = await self._generate_segmented_summary(transcript, estimated_minutes)
            
            if not summary:
                raise Exception("æ‘˜è¦ç”Ÿæˆå¤±æ•—ï¼Œè¿”å›çµæœç‚ºç©º")
            
            processing_time = time.time() - start_time
            logger.info(f"âœ¨ AIæ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {processing_time:.2f}ç§’")
            return summary
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ ç”Ÿæˆæ‘˜è¦å¤±æ•— (è€—æ™‚{processing_time:.2f}ç§’): {str(e)}")
            return "æ‘˜è¦åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œä½†éŒ„éŸ³è½‰æ–‡å­—æˆåŠŸã€‚"
    
    async def _generate_complete_summary(self, text: str) -> str:
        """å®Œæ•´æ‘˜è¦ï¼ˆçŸ­éŒ„éŸ³ï¼‰"""
        prompt = self._build_summary_prompt(text)
        
        config = types.GenerateContentConfig(
            temperature=0.1,
            max_output_tokens=60000,
            top_p=0.8,
            top_k=10
        )
        
        response = await self._call_gemini_with_rotation(prompt, config)
        return self._extract_response_text(response, text)
    
    async def _generate_focused_summary(self, text: str) -> str:
        """é‡é»æ‘˜è¦ï¼ˆä¸­ç­‰éŒ„éŸ³ï¼‰"""
        try:
            logger.info("ä½¿ç”¨é‡é»æ‘˜è¦æ¨¡å¼è™•ç†ä¸­ç­‰é•·åº¦éŒ„éŸ³")
            prompt = self._build_summary_prompt(text)
            
            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=60000,
                top_p=0.8,
                top_k=10
            )
            
            response = await self._call_gemini_with_rotation(prompt, config)
            result = self._extract_response_text(response, text)
            
            logger.info(f"é‡é»æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•·åº¦: {len(result)} å­—ç¬¦")
            return result
            
        except Exception as e:
            logger.error(f"é‡é»æ‘˜è¦ç”Ÿæˆå¤±æ•—: {e}")
            # å¦‚æœå¤±æ•—ï¼Œå˜—è©¦æ›´ç°¡å–®çš„è™•ç†æ–¹å¼
            return await self._generate_simple_focused_summary(text)
    
    async def _generate_structured_summary(self, text: str) -> str:
        """çµæ§‹åŒ–æ‘˜è¦ï¼ˆé•·éŒ„éŸ³ï¼‰"""
        # å°‡æ–‡å­—åˆ†æˆ3æ®µé€²è¡Œåˆ†æ
        length = len(text)
        segment1 = text[:length//3]
        segment2 = text[length//3:2*length//3]
        segment3 = text[2*length//3:]
        
        prompt = f"""è«‹åˆ†æä»¥ä¸‹è¼ƒé•·éŒ„éŸ³çš„å…§å®¹ï¼Œæä¾›çµæ§‹åŒ–æ‘˜è¦ï¼š

ã€å‰æ®µå…§å®¹ã€‘
{segment1[:2000]}

ã€ä¸­æ®µå…§å®¹ã€‘
{segment2[:2000]}

ã€å¾Œæ®µå…§å®¹ã€‘ 
{segment3[:2000]}

è«‹æä¾›ï¼š
1. ä¸»è¦ä¸»é¡Œ
2. é‡é»å…§å®¹
3. é—œéµçµè«–
4. é‡è¦ç´°ç¯€

è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼æä¾›è©³ç´°çš„æ‘˜è¦ï¼š

## ğŸ“ æœƒè­°/å°è©±æ‘˜è¦

### ğŸ¯ ä¸»è¦è­°é¡Œ
- [åˆ—å‡º3-5å€‹ä¸»è¦è¨è«–é»ï¼Œæ¯é»è‡³å°‘åŒ…å«2-3å¥è©³ç´°æè¿°]

### ğŸ“‹ é‡è¦å…§å®¹
- [è©³ç´°èªªæ˜é‡è¦è¨è«–å…§å®¹ï¼Œè‡³å°‘åŒ…å«5-8å€‹é—œéµé»ï¼Œæ¯é»éœ€è¦æœ‰è¶³å¤ çš„ä¸Šä¸‹æ–‡å’Œç´°ç¯€]

### âœ… è¡Œå‹•é …ç›®
- [å¦‚æœæœ‰çš„è©±ï¼Œåˆ—å‡ºéœ€è¦åŸ·è¡Œçš„äº‹é …ï¼ŒåŒ…å«è² è²¬äººå’Œæ™‚é–“ç·š]

### ğŸ’¡ é—œéµæ´å¯Ÿ
- [ç¸½çµé‡è¦çš„è¦‹è§£æˆ–çµè«–ï¼Œè‡³å°‘3-5é»ï¼Œæ¯é»åŒ…å«å…·é«”ä¾æ“š]

### ğŸ”‘ é—œéµå­—
[ç›¸é—œé—œéµå­—ç”¨é€—è™Ÿåˆ†éš”ï¼Œè‡³å°‘10-15å€‹é—œéµè©]

è«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼Œå…§å®¹å¿…é ˆè©³ç´°ä¸”æœ‰çµ„ç¹”æ€§ã€‚"""

        config = types.GenerateContentConfig(
            temperature=0.1,
            max_output_tokens=60000,
            top_p=0.8,
            top_k=10
        )
        
        response = await self._call_gemini_with_rotation(prompt, config)
        result = self._extract_response_text(response, text, structured=True)
        
        return f"{result}\n\nğŸ“Š éŒ„éŸ³æ™‚é•·ï¼šç´„ {len(text)/180:.0f} åˆ†é˜"
    
    async def _generate_segmented_summary(self, text: str, estimated_minutes: float) -> str:
        """åˆ†æ®µå¼æ‘˜è¦ï¼ˆè¶…é•·éŒ„éŸ³ï¼‰"""
        try:
            # å°‡æ–‡å­—åˆ†æˆå¤šå€‹æ®µè½ï¼Œæ¯æ®µç´„3000å­—
            segments = []
            chunk_size = 3000
            for i in range(0, len(text), chunk_size):
                segment = text[i:i+chunk_size]
                segments.append(segment)
            
            logger.info(f"è¶…é•·éŒ„éŸ³åˆ†ç‚º {len(segments)} æ®µè™•ç†")
            
            # æ ¹æ“šé…ç½®æ±ºå®šåˆ†æå¤šå°‘æ®µè½
            full_analysis = getattr(self.config, 'full_analysis', False)
            max_segments = getattr(self.config, 'max_segments_for_full_analysis', 10)
            
            if full_analysis:
                # å®Œæ•´åˆ†ææ‰€æœ‰æ®µè½
                if len(segments) <= max_segments:
                    key_segments = segments
                    analysis_note = f"ï¼ˆå®Œæ•´åˆ†æ {len(segments)} æ®µï¼‰"
                    logger.info(f"é€²è¡Œå®Œæ•´åˆ†æï¼Œè™•ç† {len(segments)} æ®µ")
                else:
                    # å¦‚æœæ®µè½æ•¸è¶…éé™åˆ¶ï¼Œé€²è¡Œè­¦å‘Šä½†ä»ç›¡å¯èƒ½åˆ†ææ›´å¤š
                    key_segments = segments[:max_segments]
                    analysis_note = f"ï¼ˆå› æ®µè½éå¤šï¼Œå·²åˆ†æå‰ {len(key_segments)} æ®µï¼Œå…± {len(segments)} æ®µï¼‰"
                    logger.warning(f"æ®µè½æ•¸ {len(segments)} è¶…éé™åˆ¶ {max_segments}ï¼Œåªåˆ†æå‰ {len(key_segments)} æ®µ")
            else:
                # æ™ºèƒ½é¸å–é—œéµæ®µè½
                if len(segments) > 10:
                    # å–é–‹é ­3æ®µã€ä¸­é–“2æ®µã€çµå°¾3æ®µ
                    key_segments = segments[:3] + segments[len(segments)//2-1:len(segments)//2+1] + segments[-3:]
                    analysis_note = f"ï¼ˆæ™ºèƒ½é¸å–ï¼šå·²å¾ {len(segments)} æ®µä¸­é¸å– {len(key_segments)} å€‹é—œéµæ®µè½åˆ†æï¼‰"
                else:
                    key_segments = segments[:6]  # æœ€å¤šè™•ç†å‰6æ®µ
                    analysis_note = f"ï¼ˆå…± {len(segments)} æ®µï¼Œå·²åˆ†æå‰ {len(key_segments)} æ®µï¼‰"
            
            # ç”Ÿæˆåˆ†æ®µæ‘˜è¦
            segment_summaries = []
            total_segments = len(key_segments)
            
            # å¦‚æœæ˜¯å®Œæ•´åˆ†æä¸”æ®µè½å¾ˆå¤šï¼Œç™¼é€é€²åº¦é€šçŸ¥
            if full_analysis and total_segments > 20:
                logger.info(f"é–‹å§‹å®Œæ•´åˆ†æ {total_segments} æ®µï¼Œé è¨ˆéœ€è¦ {total_segments * 0.5:.0f} ç§’")
            
            for i, segment in enumerate(key_segments):
                try:
                    # å‹•æ…‹èª¿æ•´æ®µè½æ¨™è¨˜ï¼ˆå¦‚æœæ˜¯æ™ºèƒ½é¸å–ï¼Œä½¿ç”¨åŸå§‹æ®µè½è™Ÿï¼‰
                    if full_analysis or len(segments) <= 10:
                        segment_label = f"ç¬¬{i+1}æ®µ"
                    else:
                        # æ™ºèƒ½é¸å–æ¨¡å¼ï¼Œè¨ˆç®—åŸå§‹æ®µè½è™Ÿ
                        if i < 3:
                            segment_number = i + 1
                        elif i < 5:
                            segment_number = len(segments)//2 + (i - 3)
                        else:
                            segment_number = len(segments) - (7 - i)
                        segment_label = f"ç¬¬{segment_number}æ®µ"
                    
                    prompt = f"è«‹ç°¡æ½”ç¸½çµä»¥ä¸‹éŒ„éŸ³ç‰‡æ®µçš„é‡é»ï¼ˆ{segment_label}ï¼‰ï¼š\n\n{segment[:2000]}"
                    
                    config = types.GenerateContentConfig(
                        temperature=0.1,
                        max_output_tokens=10000,
                        top_p=0.8,
                        top_k=5
                    )
                    
                    response = await self._call_gemini_with_rotation(prompt, config)
                    if response and hasattr(response, 'candidates') and response.candidates:
                        summary = response.text.strip()
                        segment_summaries.append(f"ã€{segment_label}ã€‘{summary}")
                    
                    # è¨˜éŒ„è™•ç†é€²åº¦
                    if (i + 1) % 10 == 0:
                        logger.info(f"å·²å®Œæˆ {i + 1}/{total_segments} æ®µåˆ†æ")
                    
                    delay = getattr(self.config, 'segment_processing_delay', 0.5)
                    await asyncio.sleep(delay)  # ä½¿ç”¨é…ç½®çš„å»¶é²æ™‚é–“
                    
                except Exception as e:
                    logger.warning(f"è™•ç†{segment_label}æ™‚å‡ºéŒ¯: {e}")
                    segment_summaries.append(f"ã€{segment_label}ã€‘è™•ç†å¤±æ•—")
            
            # ç”Ÿæˆç¸½é«”æ‘˜è¦
            combined_summary = "\n\n".join(segment_summaries)
            
            final_prompt = f"""åŸºæ–¼ä»¥ä¸‹åˆ†æ®µæ‘˜è¦ï¼Œè«‹æä¾›æ•´é«”é‡é»ç¸½çµï¼š

{combined_summary}

è«‹æä¾›ï¼š
1. ä¸»è¦è­°é¡Œå’Œä¸»é¡Œ
2. æ ¸å¿ƒè§€é»å’Œçµè«–
3. é‡è¦æ±ºå®šæˆ–è¡Œå‹•é …ç›®

è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼æä¾›æ‘˜è¦ï¼š

## ğŸ“ æœƒè­°/å°è©±æ‘˜è¦

### ğŸ¯ ä¸»è¦è­°é¡Œ
- [åˆ—å‡º3-5å€‹ä¸»è¦è¨è«–é»ï¼Œæ¯é»åŒ…å«è©³ç´°æè¿°]

### ğŸ“‹ é‡è¦å…§å®¹
- [è©³ç´°èªªæ˜é‡è¦è¨è«–å…§å®¹]

### âœ… è¡Œå‹•é …ç›®
- [å¦‚æœæœ‰çš„è©±ï¼Œåˆ—å‡ºéœ€è¦åŸ·è¡Œçš„äº‹é …]

### ğŸ’¡ é—œéµæ´å¯Ÿ
- [ç¸½çµé‡è¦çš„è¦‹è§£æˆ–çµè«–]

### ğŸ”‘ é—œéµå­—
[ç›¸é—œé—œéµå­—ç”¨é€—è™Ÿåˆ†éš”]"""

            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=60000,
                top_p=0.8,
                top_k=10
            )
            
            final_response = await self._call_gemini_with_rotation(final_prompt, config)
            final_summary = self._extract_response_text(final_response, text, structured=True)
            
            # çµ„åˆæœ€çµ‚çµæœ
            result = f"ğŸ¯ ã€æ•´é«”æ‘˜è¦ã€‘\n{final_summary}\n\nğŸ“ ã€åˆ†æ®µé‡é»ã€‘\n{combined_summary}\n\n"
            result += f"â±ï¸ éŒ„éŸ³æ™‚é•·ï¼šç´„ {estimated_minutes:.0f} åˆ†é˜ ({len(text)} å­—)\n"
            result += f"ğŸ“Š åˆ†æèªªæ˜ï¼š{analysis_note}"
            
            return result
            
        except Exception as e:
            logger.error(f"åˆ†æ®µæ‘˜è¦è™•ç†å¤±æ•—: {e}")
            return await self._generate_fallback_summary(text, estimated_minutes)
    
    async def _generate_fallback_summary(self, text: str, estimated_minutes: float) -> str:
        """å‚™ç”¨æ‘˜è¦ï¼ˆç•¶åˆ†æ®µè™•ç†å¤±æ•—æ™‚ï¼‰"""
        # åªå–é–‹é ­å’Œçµå°¾é€²è¡Œæ‘˜è¦
        start_text = text[:2000]
        end_text = text[-2000:] if len(text) > 4000 else ""
        
        summary_text = f"é–‹é ­ï¼š{start_text}"
        if end_text:
            summary_text += f"\n\nçµå°¾ï¼š{end_text}"
        
        prompt = f"é€™æ˜¯ä¸€å€‹ç´„ {estimated_minutes:.0f} åˆ†é˜çš„é•·éŒ„éŸ³çš„é–‹é ­å’Œçµå°¾éƒ¨åˆ†ï¼Œè«‹æä¾›åŸºæœ¬æ‘˜è¦ï¼š\n\n{summary_text}"
        
        try:
            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=30000,
                top_p=0.8,
                top_k=5
            )
            
            response = await self._call_gemini_with_rotation(prompt, config)
            result = self._extract_response_text(response, text)
            
            return f"{result}\n\nâš ï¸ å› éŒ„éŸ³éé•·ï¼Œæ­¤ç‚ºç°¡åŒ–æ‘˜è¦\nâ±ï¸ éŒ„éŸ³æ™‚é•·ï¼šç´„ {estimated_minutes:.0f} åˆ†é˜"
            
        except Exception as e:
            logger.error(f"å‚™ç”¨æ‘˜è¦ä¹Ÿå¤±æ•—: {e}")
            return f"âœ… éŒ„éŸ³è½‰æ–‡å­—æˆåŠŸ\nâ±ï¸ éŒ„éŸ³æ™‚é•·ï¼šç´„ {estimated_minutes:.0f} åˆ†é˜ ({len(text)} å­—)\nğŸ“ å› å…§å®¹éé•·ï¼Œæ‘˜è¦åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œè«‹æŸ¥çœ‹å®Œæ•´é€å­—ç¨¿"
    
    async def _generate_simple_focused_summary(self, text: str) -> str:
        """ç°¡åŒ–ç‰ˆé‡é»æ‘˜è¦ï¼ˆä¸­ç­‰éŒ„éŸ³å‚™ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            logger.info("ä½¿ç”¨ç°¡åŒ–ç‰ˆé‡é»æ‘˜è¦")
            # åˆ†æ®µè™•ç†ï¼Œæ¯æ®µ2000å­—ç¬¦
            chunks = [text[i:i+2000] for i in range(0, len(text), 2000)]
            
            summaries = []
            for i, chunk in enumerate(chunks[:3]):  # æœ€å¤šè™•ç†å‰3æ®µ
                try:
                    prompt = f"è«‹ç°¡æ½”ç¸½çµä»¥ä¸‹å…§å®¹çš„é‡é»ï¼š\n\n{chunk}"
                    
                    config = types.GenerateContentConfig(
                        temperature=0.1,
                        max_output_tokens=20000,
                        top_p=0.8,
                        top_k=5
                    )
                    
                    response = await self._call_gemini_with_rotation(prompt, config)
                    if (response and hasattr(response, 'candidates') and response.candidates and 
                        "STOP" in str(response.candidates[0].finish_reason)):
                        summaries.append(response.text.strip())
                    
                    await asyncio.sleep(0.3)  # çŸ­æš«å»¶é²
                    
                except Exception as e:
                    logger.warning(f"è™•ç†ç¬¬{i+1}æ®µç°¡åŒ–æ‘˜è¦å¤±æ•—: {e}")
                    continue
            
            if summaries:
                result = "\n\n".join(summaries)
                if len(chunks) > 3:
                    result += f"\n\nğŸ’¡ è¨»ï¼šå·²æ‘˜è¦å‰3æ®µå…§å®¹ï¼Œç¸½å…±{len(chunks)}æ®µ"
                return result
            else:
                return await self._generate_short_summary(text[:1000])
                
        except Exception as e:
            logger.error(f"ç°¡åŒ–ç‰ˆé‡é»æ‘˜è¦å¤±æ•—: {e}")
            return await self._generate_short_summary(text[:1000])
    
    async def _generate_short_summary(self, text: str) -> str:
        """ç”Ÿæˆç°¡çŸ­æ‘˜è¦ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
        try:
            logger.info("ä½¿ç”¨ç°¡çŸ­æ‘˜è¦æ¨¡å¼")
            prompt = f"è«‹ç”¨æœ€ç°¡æ½”çš„æ–¹å¼ç¸½çµä»¥ä¸‹å…§å®¹çš„ä¸»è¦é‡é»ï¼ˆé™100å­—å…§ï¼‰ï¼š\n\n{text[:1000]}"
            
            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=20000,
                top_p=0.8,
                top_k=5
            )

            response = await self._call_gemini_with_rotation(prompt, config)
            
            if (response and hasattr(response, 'candidates') and response.candidates and 
                "STOP" in str(response.candidates[0].finish_reason)):
                return f"{response.text.strip()}\n\nâš ï¸ å› è™•ç†é™åˆ¶ï¼Œæ­¤ç‚ºç°¡åŒ–æ‘˜è¦"
            else:
                return "âœ… éŒ„éŸ³è½‰æ–‡å­—æˆåŠŸ\nğŸ“ å…§å®¹è¼ƒé•·ï¼Œå»ºè­°æŸ¥çœ‹å®Œæ•´é€å­—ç¨¿"
                
        except Exception as e:
            logger.error(f"ç°¡çŸ­æ‘˜è¦ä¹Ÿå¤±æ•—: {e}")
            return "âœ… éŒ„éŸ³è½‰æ–‡å­—æˆåŠŸ\nğŸ“ æ‘˜è¦åŠŸèƒ½æš«æ™‚ç„¡æ³•ä½¿ç”¨ï¼Œè«‹æŸ¥çœ‹å®Œæ•´é€å­—ç¨¿"
    
    def _build_summary_prompt(self, transcript: str) -> str:
        """æ§‹å»ºæ‘˜è¦æç¤ºè©"""
        return f"""
è«‹ç‚ºä»¥ä¸‹éŒ„éŸ³è½‰éŒ„å…§å®¹ç”Ÿæˆä¸€å€‹è©³ç´°ä¸”çµæ§‹åŒ–çš„æ‘˜è¦ã€‚æ‘˜è¦é•·åº¦æ‡‰è©²èˆ‡åŸæ–‡é•·åº¦æˆæ¯”ä¾‹ï¼Œç¢ºä¿åŒ…å«æ‰€æœ‰é‡è¦ä¿¡æ¯ã€‚

è½‰éŒ„å…§å®¹ï¼š
{transcript}

è«‹æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç”Ÿæˆè©³ç´°çš„æ‘˜è¦ï¼š

## ğŸ“ æœƒè­°/å°è©±æ‘˜è¦

### ğŸ¯ ä¸»è¦è­°é¡Œ
- [åˆ—å‡º3-5å€‹ä¸»è¦è¨è«–é»ï¼Œæ¯é»è‡³å°‘åŒ…å«2-3å¥è©³ç´°æè¿°]

### ğŸ“‹ é‡è¦å…§å®¹
- [è©³ç´°èªªæ˜é‡è¦è¨è«–å…§å®¹ï¼Œè‡³å°‘åŒ…å«5-8å€‹é—œéµé»ï¼Œæ¯é»éœ€è¦æœ‰è¶³å¤ çš„ä¸Šä¸‹æ–‡å’Œç´°ç¯€]

### âœ… è¡Œå‹•é …ç›®
- [å¦‚æœæœ‰çš„è©±ï¼Œåˆ—å‡ºéœ€è¦åŸ·è¡Œçš„äº‹é …ï¼ŒåŒ…å«è² è²¬äººå’Œæ™‚é–“ç·š]

### ğŸ’¡ é—œéµæ´å¯Ÿ
- [ç¸½çµé‡è¦çš„è¦‹è§£æˆ–çµè«–ï¼Œè‡³å°‘3-5é»ï¼Œæ¯é»åŒ…å«å…·é«”ä¾æ“š]

### ğŸ”‘ é—œéµå­—
[ç›¸é—œé—œéµå­—ç”¨é€—è™Ÿåˆ†éš”ï¼Œè‡³å°‘10-15å€‹é—œéµè©]

è«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼Œå…§å®¹å¿…é ˆè©³ç´°ä¸”æœ‰çµ„ç¹”æ€§ã€‚æ‘˜è¦çš„å­—æ•¸æ‡‰è©²åˆç†åæ˜ åŸå§‹æ–‡æœ¬çš„ä¿¡æ¯é‡ï¼Œé¿å…éæ–¼ç°¡çŸ­ã€‚å°æ–¼ä¸€è¬å­—ä»¥ä¸Šçš„æ–‡æœ¬ï¼Œæ‘˜è¦è‡³å°‘æ‡‰æœ‰1000å­—ä»¥ä¸Šã€‚
"""
    
    async def _call_gemini_with_rotation(self, prompt: str, config: types.GenerateContentConfig):
        """ç•°æ­¥èª¿ç”¨Gemini APIï¼Œä¸¦åœ¨å¤±æ•—æ™‚è¼ªæ›APIé‡‘é‘°"""
        last_error = None
        
        for attempt in range(self.max_retries + 1):
            try:
                client = self.genai_clients[self.current_genai_index]
                
                # ä½¿ç”¨å®˜æ–¹SDKèª¿ç”¨API
                response = client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=config
                )
                
                return response
                
            except Exception as e:
                last_error = e
                logger.warning(f"âš ï¸ Gemini APIèª¿ç”¨å˜—è©¦ {attempt + 1} å¤±æ•— (ä½¿ç”¨é‡‘é‘° {self.current_genai_index + 1}): {str(e)}")
                
                # åˆ‡æ›åˆ°ä¸‹ä¸€å€‹APIé‡‘é‘°
                self.current_genai_index = (self.current_genai_index + 1) % len(self.genai_clients)
                
                if attempt < self.max_retries:
                    await asyncio.sleep(2 ** attempt)  # æŒ‡æ•¸é€€é¿
                    continue
                else:
                    break
        
        raise last_error or Exception("æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†")
    
    def _extract_response_text(self, response, original_text: str, structured: bool = False) -> str:
        """æå–å›æ‡‰æ–‡å­—ä¸¦è™•ç†å„ç¨®ç‹€æ³"""
        if not response or not hasattr(response, 'candidates') or not response.candidates:
            logger.warning("Gemini å›æ‡‰ç„¡å…§å®¹æˆ–ç„¡å€™é¸é …")
            raise Exception("ç„¡æ³•ç”Ÿæˆæ‘˜è¦å›æ‡‰")
        
        candidate = response.candidates[0]
        finish_reason = str(candidate.finish_reason) if hasattr(candidate, 'finish_reason') else "UNKNOWN"
        
        logger.info(f"Gemini å›æ‡‰ç‹€æ…‹: {finish_reason}")
        
        if "STOP" in finish_reason:
            result = response.text.strip()
            logger.info(f"æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•·åº¦: {len(result)} å­—ç¬¦")
            return result
        elif "SAFETY" in finish_reason:
            return "âš ï¸ å…§å®¹å¯èƒ½åŒ…å«æ•æ„Ÿè³‡è¨Šï¼Œç„¡æ³•ç”¢ç”Ÿæ‘˜è¦"
        elif "MAX_TOKEN" in finish_reason or "LENGTH" in finish_reason:
            logger.warning(f"Token é™åˆ¶è§¸ç™¼: {finish_reason}")
            # å¦‚æœæ˜¯çµæ§‹åŒ–è™•ç†ï¼Œå˜—è©¦è¿”å›éƒ¨åˆ†çµæœ
            if structured and response.text:
                return f"{response.text.strip()}\n\nâš ï¸ æ‘˜è¦å› é•·åº¦é™åˆ¶å¯èƒ½ä¸å®Œæ•´"
            else:
                # å°æ–¼ä¸­ç­‰é•·åº¦éŒ„éŸ³ï¼Œå˜—è©¦ç°¡åŒ–è™•ç†
                raise Exception(f"å…§å®¹éé•·éœ€è¦ç°¡åŒ–è™•ç†: {finish_reason}")
        else:
            logger.warning(f"æœªçŸ¥çš„å®Œæˆç‹€æ…‹: {finish_reason}")
            if response.text and len(response.text.strip()) > 0:
                return f"{response.text.strip()}\n\nâš ï¸ æ‘˜è¦å¯èƒ½ä¸å®Œæ•´ï¼ˆ{finish_reason}ï¼‰"
            else:
                raise Exception(f"æ‘˜è¦ç”Ÿæˆç•°å¸¸: {finish_reason}")
    
    async def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """åˆ†ææ–‡å­—æƒ…æ„Ÿ"""
        try:
            prompt = f"""
åˆ†æä»¥ä¸‹æ–‡å­—çš„æƒ…æ„Ÿå‚¾å‘ï¼Œè«‹è¿”å›JSONæ ¼å¼ï¼š

æ–‡å­—å…§å®¹ï¼š
{text}

è«‹åˆ†æä¸¦è¿”å›ï¼š
{{
    "overall_sentiment": "positive/negative/neutral",
    "confidence": 0.0-1.0,
    "emotions": ["å…·é«”æƒ…æ„Ÿæ¨™ç±¤"],
    "key_phrases": ["é—œéµçŸ­èª"]
}}
"""
            
            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=20000,
                top_p=0.8,
                top_k=5
            )
            
            response = await self._call_gemini_with_rotation(prompt, config)
            result = response.text.strip()
            
            # å˜—è©¦è§£æç‚ºJSON
            try:
                return json.loads(result)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯æœ‰æ•ˆJSONï¼Œè¿”å›åŸºæœ¬æ ¼å¼
                return {
                    "overall_sentiment": "neutral",
                    "confidence": 0.5,
                    "emotions": [],
                    "key_phrases": [],
                    "raw_response": result
                }
                
        except Exception as e:
            logger.error(f"æƒ…æ„Ÿåˆ†æå¤±æ•—: {str(e)}")
            raise
    
    async def extract_keywords(self, text: str) -> List[str]:
        """æå–é—œéµå­—"""
        try:
            prompt = f"""
å¾ä»¥ä¸‹æ–‡å­—ä¸­æå–æœ€é‡è¦çš„é—œéµå­—ï¼Œæ¯è¡Œä¸€å€‹é—œéµå­—ï¼Œæœ€å¤š15å€‹ï¼š

{text}

é—œéµå­—ï¼š
"""
            
            config = types.GenerateContentConfig(
                temperature=0.1,
                max_output_tokens=20000,
                top_p=0.8,
                top_k=5
            )
            
            response = await self._call_gemini_with_rotation(prompt, config)
            result = response.text.strip()
            
            # è§£æé—œéµå­—
            keywords = []
            for line in result.split('\n'):
                keyword = line.strip('- ').strip()
                if keyword and len(keyword) > 1:
                    keywords.append(keyword)
            
            return keywords[:15]  # æœ€å¤šè¿”å›15å€‹é—œéµå­—
            
        except Exception as e:
            logger.error(f"é—œéµå­—æå–å¤±æ•—: {str(e)}")
            return []
    
    async def check_status(self) -> Dict[str, Any]:
        """æª¢æŸ¥æœå‹™ç‹€æ…‹"""
        try:
            client = self.genai_clients[self.current_genai_index]
            
            # ä½¿ç”¨ä¸€å€‹ç°¡å–®çš„æç¤ºè©æ¸¬è©¦æœå‹™æ˜¯å¦å¯ç”¨
            try:
                response = client.models.generate_content(
                    model=self.model,
                    contents="æ¸¬è©¦",
                    config=types.GenerateContentConfig(
                        temperature=0.1,
                        max_output_tokens=10,
                    )
                )
                
                if response and hasattr(response, 'candidates') and response.candidates:
                    return {
                        "available": True,
                        "model": self.model,
                        "provider": "google_gemini",
                        "api_keys_count": len(self.api_keys),
                        "using_sdk": True
                    }
                else:
                    return {
                        "available": False,
                        "error": "APIéŸ¿æ‡‰ç„¡å…§å®¹"
                    }
            except Exception as e:
                return {
                    "available": False,
                    "error": str(e)
                }
                
        except Exception as e:
            return {
                "available": False,
                "error": str(e)
            } 